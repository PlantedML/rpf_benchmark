---
title: "RPF Classification Benchmark Results"
author: "Lukas"
date: now
date-format: "YYYY-MM-DD HH:mm z"
date-modified: last-modified
format: 
  html:
    code-fold: true
    fig-align: center
    toc: true
  pdf: 
    documentclass: article
    fig-align: center
    toc: true
    date: now
    keep-tex: true
    knitr: 
      opts_chunk: 
        echo: false
        fig.pos: H
    include-in-header: 
      - \usepackage{float}
editor_options: 
  chunk_output_type: console
---

```{r setup}
#| message: false
#| warning: false
library(ggplot2)
library(mlr3)
#library(mlr3batchmark)
#library(mlr3viz)
#library(mlr3tuning)
library(data.table)
library(dplyr)
library(kableExtra)

# measures for evaluation
msr_binary_auc <- msr("classif.auc", id = "AUC")
msr_binary_brier <- msr("classif.bbrier", id = "Brier Score")
msr_multiclass_auc <- msr("classif.mauc_aunp", id = "AUC")
msr_multiclass_brier <- msr("classif.mbrier", id = "Brier Score")

# Load task metadata
task_summary <- as.data.table(readRDS("task_summary.rds"))

# benchmark results + cleanup
bmr_binary_auc   <- readRDS("data/bmr-binary-auc.rds")
bmr_binary_brier <- readRDS("data/bmr-binary-brier.rds")
bmr_multiclass_auc <- readRDS("data/bmr-multiclass-auc.rds")
bmr_multiclass_brier <- readRDS("data/bmr-multiclass-brier.rds")

collect_aggr <- function(bmr, tuning_measure) {
  # Eval on same measure as was tuned on
  aggr <- bmr$aggregate(measures = tuning_measure)
  # rename learners for convenience
  aggr[, learner_name := gsub("\\.tuned", "", gsub("(encode\\.)?classif\\.", "", learner_id))]
  aggr[, c("iters", "resampling_id", "resample_result", "nr", "learner_id") := NULL]
  # long version for plotting over multiple measures
  aggr_long <- melt(aggr, measure.vars = tuning_measure$id, variable.name = "measure", value.name = "score")
  aggr_long[, measure := gsub("^classif\\.", "", measure)]
  aggr_long[]
}

collect_scores <- function(bmr, tuning_measure, task_summary) {
  scores <- bmr$score(measures = tuning_measure)
  scores[, learner_name := gsub("\\.tuned", "", gsub("(encode\\.)?classif\\.", "", learner_id))]
  # Remove unneeded columns
  scores[, c("task", "learner", "resampling", "resampling_id") := NULL]
  # Join task metadata
  scores <- scores[task_summary[, -c("task_id")], on = c("task_id" = "task_name_full"), nomatch = 0]
  
  # Sort task_name levels by n * p from task_summary
  scores[, task_name := factor(task_name, levels = task_summary$task_name)]
  scores_long <- melt(scores, measure.vars = tuning_measure$id, variable.name = "measure", value.name = "score")
  scores_long[, measure := gsub("^classif\\.", "", measure)][]
}


binary_aggrs <- rbindlist(list(
 collect_aggr(bmr_binary_auc,   msr_binary_auc),
 collect_aggr(bmr_binary_brier, msr_binary_brier)
))

binary_scores <- rbindlist(list(
 collect_scores(bmr_binary_auc,   msr_binary_auc,   task_summary),
 collect_scores(bmr_binary_brier, msr_binary_brier, task_summary)
))

multiclass_aggrs <- rbindlist(list(
 collect_aggr(bmr_multiclass_auc,   msr_multiclass_auc),
 collect_aggr(bmr_multiclass_brier, msr_multiclass_brier)
))

multiclass_scores <- rbindlist(list(
 collect_scores(bmr_multiclass_auc,   msr_multiclass_auc,   task_summary),
 collect_scores(bmr_multiclass_brier, msr_multiclass_brier, task_summary)
))

# Tuning results cleanup 
cleanup_tuning_results <- function(results, task_summary) {
  results <- copy(results)
  # merge with task summary for task metadata/names
  results <- results[task_summary[, -c("task_id")], on = c("task_id" = "task_name_full"), nomatch = 0]
  # Cleanup learner names
  results[, learner_name := gsub("\\.tuned", "", gsub("(encode\\.)?classif\\.", "", learner_id))]
  # Reconstruct max_interaction from _ratio, using p from task_summary
  # taking into account the max_interaction_limit of 20
  results[, max_interaction := ifelse(learner_id == "classif.rpf_fixmax.tuned", 2, pmax(ceiling(max_interaction_ratio * pmin(p, 30)), 1))]
  results[, task_name := factor(task_name, levels = task_summary$task_name)]
  results[]
}

tuning_results_rpf_binary_auc   <- cleanup_tuning_results(readRDS("data/tuning_results_rpf-binary-auc.rds"),   task_summary = task_summary)
tuning_results_rpf_binary_brier <- cleanup_tuning_results(readRDS("data/tuning_results_rpf-binary-brier.rds"), task_summary = task_summary)

tuning_results_rpf_multiclass_auc   <- cleanup_tuning_results(readRDS("data/tuning_results_rpf-multiclass-auc.rds"), task_summary = task_summary)
tuning_results_rpf_multiclass_brier <- cleanup_tuning_results(readRDS("data/tuning_results_rpf-multiclass-brier.rds"), task_summary = task_summary)

# Define learner colors for somewhat identifiable plots
learner_cols <- c(
  "rpf" = "#F73098",
  "xgboost" = "#3BA99C",
  "rpf_fixmax" = "#CA1694",
  "xgboost_fixdepth" = "#256A62",
  "ranger" = "#2171B5"
)
learner_order <- names(learner_cols)
# learner_label <- function(x) {
#   
# }

set.seed(3) # Only for jitterdodge consistency
```

## Benchmark Setup

- Inner resampling folds: 5
- Outer resampling folds: 10
- Random search with 200 evaluations

### Hyperparameter Search Spaces

### `rpf`/`rpf_fixmax`

- `ntrees` := 50
- `max_interaction` in [1, 30]
    - Via `max_interaction_ratio` in [0, 1] and `max_interaction_limit` := 30
    - for `rpf_fixmax`: `max_interaction` := 2
- `loss` in {L1, L2, exponential}
- `splits` in [10, 100]
- `splits_try` in [1, 20]
- `t_try` in [0.1, 1]

### `ranger`

- `num.trees` := 50
- `mtry_ratio` in [0.1, 1]
- `min.node.size` in [1, 50]
- `replace` in {TRUE, FALSE}
- `sample.fracton` in [0.1, 1]

### `xgboost`/`xgboost_fixdepth`

- Preprocessing of categorical features using treatment encoding
- `max_depth` in [1, 20]
    - for `xgb_fixdepth`: `max_depth` := 2
- `subsample` in [0.1, 1]
- `colsample_bytree` in [0.1, 1]
- `nrounds` in [10, 5000]
- `eta` in [0, 1]


## Binary Classification

- Tuning and evaluation on:
    -  [AUC](https://mlr3.mlr-org.com/reference/mlr_measures_classif.auc.html)
    -  [Binary Brier](https://mlr3.mlr-org.com/reference/mlr_measures_classif.bbrier.html)

### Tasks

[OpenML CC18](https://www.openml.org/search?type=study&study_type=task&id=99) tasks with:

- Binary target
- No missing values
- $n \cdot p \leq 10^5$

```{r binary-tasks}
task_summary |>
  filter(twoclass & dim <= 1e5) |>
  select(task_id, task_name, n, p, dim) |>
  kbl(
    caption = "Selected OpenML CC18 tasks for binary classification",
    col.names = c("ID", "Task", "n", "p", "np"),
    booktabs = TRUE
  ) |>
  kable_styling()
```


### Aggregated Results

#### Boxplot

```{r binary-aggregated}
#| fig-width: 12
#| fig-height: 7
#| fig-align: center
#| column: screen-inset-shaded

binary_aggrs |>
  mutate(learner_name = factor(learner_name, levels = learner_order)) |>
  ggplot(aes(x = learner_name, y = score, fill = learner_name)) +
    facet_wrap(vars(measure), ncol = 2, scales = "free_y") +
    geom_boxplot(alpha = .5) +
    geom_point(position = position_jitterdodge(dodge.width = .25)) +
    scale_y_continuous(labels = scales::percent) +
    #scale_x_discrete(guide = guide_axis(n.dodge = 2)) +
    #coord_cartesian(ylim = c(0, 1)) +
    #coord_flip() +
    scale_fill_manual(values = learner_cols, guide = "none") +
    labs(
      title = "Binary Classification: Aggregated scores over all tasks",
      x = NULL, y = "Score"
    ) +
    theme_minimal(base_size = 14)
```

#### Table

```{r binary-aggregated-table}
# column: screen-inset-shaded
binary_aggrs |>
  group_by(learner_name, measure) |>
  summarize(
    mean = mean(score),
    median = median(score),
    sd = sd(score),
    q25 = quantile(score, .25),
    q75 = quantile(score, .75),
    .groups = "drop"
  ) |>
  mutate(across(where(is.numeric), \(x) round(100 * x, 1))) |>
  mutate(
    mean = glue::glue("{mean} [{sd}]"),
    median = glue::glue("{median} [{q25}; {q75}]")
  ) |>
  select(-sd, -q25, -q75) |>
  tidyr::pivot_wider(names_from = measure, names_sep = "_", values_from = mean:median) |>
  select(learner_name, contains("AUC"), contains("Brier")) |>
  kbl(
    col.names = c("Learner", rep(c("Mean [SD]", "Median [q25; q75]"), 2)),
    booktabs = TRUE
  ) |>
  kable_styling(full_width = TRUE, latex_options = c("HOLD_position")) |>
  add_header_above(header = c(" " = 1, "AUC (%)" = 2, "Brier Score (%)" = 2)) |>
  column_spec(1, "4cm") |>
  column_spec(2, "2.3cm") |>
  column_spec(3, "3cm") |>
  column_spec(4, "2.3cm") |>
  column_spec(5, "3cm")
```


### Results per Task

```{r binary-per-task-auc}
#| fig-width: 12
#| fig-height: 7
#| fig-align: center
#| column: screen-inset-shaded
#| 
binary_scores |>
  filter(measure == "AUC") |>
  ggplot(aes(y = learner_name, x = score, fill = learner_name)) +
  facet_wrap(vars(task_name)) +
  geom_boxplot(alpha = .5) +
  geom_point() +
  scale_fill_manual(values = learner_cols, guide = "none") +
  scale_x_continuous(labels = scales::percent) +
  labs(
    title = "Binary classification scores per task",
    subtitle = "Tasks ordered by n * p, decreasing",
    y = "Learner", x = "AUC (%)"
  ) +
  theme_minimal() +
  theme(
    panel.spacing.x = unit(1, "cm")
  )
```


```{r binary-per-task-brier}
#| fig-width: 12
#| fig-height: 7
#| fig-align: center
#| column: screen-inset-shaded

binary_scores |>
  filter(measure == "Brier Score") |>
  ggplot(aes(y = learner_name, x = score, fill = learner_name)) +
  facet_wrap(vars(task_name)) +
  geom_boxplot(alpha = .5) +
  geom_point() +
  scale_fill_manual(values = learner_cols, guide = "none") +
  scale_x_continuous(labels = scales::percent) +
  labs(
    title = "Binary classification scores per task",
    subtitle = "Tasks ordered by n * p, decreasing",
    y = "Learner", x = "Brier Score (%)"
  ) +
  theme_minimal() +
  theme(
    panel.spacing.x = unit(1, "cm")
  )
```

### Tuning Results

Showing best performing parameter combinations of the inner folds, e.g. the parameters that "won".
One plot per hyperparameter, with performance on y-axis.

Note that `max_interaction` is tuned by tuning `max_interaction_ratio` within [0, 1] and then calculating

$$\max(\lceil \mathtt{max\_interaction\_ratio} \cdot \min(p, 30)\rceil, 1)$$

to effectively tune `max_interaction` within 1 and $p$ or 30, whichever is lower. 
This is not ideal since multiple values for `max_interaction_ratio` could result in the same value for `max_interaction` for large $p$, but it's a compromise.

```{r binary-tuning-results-auc}
#| fig-width: 12
#| fig-height: 7
#| fig-align: center
#| column: screen-inset-shaded

tuning_results_rpf_binary_auc |>
  ggplot(aes(x = splits, y = classif.auc, color = learner_name)) +
  facet_wrap(vars(task_name)) +
  geom_point(size = 2, alpha = .75) +
  scale_color_brewer(palette = "Dark2") +
  labs(
    title = "Binary classifification tuning results",
    subtitle = "Tuned on AUC"
  ) +
  theme_minimal() +
  theme(
    panel.spacing.x = unit(1, "cm"),
    legend.position = "bottom"
  )

tuning_results_rpf_binary_auc |>
 # mutate(task_name = glue::glue("{task_name} (p={p})")) |>
  ggplot(aes(x = max_interaction, y = classif.auc, color = learner_name)) +
  facet_wrap(vars(task_name)) +
  geom_point(size = 2, alpha = .75) +
  scale_color_brewer(palette = "Dark2") +
  labs(
    title = "Binary classifification tuning results",
    subtitle = "Tuned on AUC"
  ) +
  theme_minimal() +
  theme(
    panel.spacing.x = unit(1, "cm"),
    legend.position = "bottom"
  )

tuning_results_rpf_binary_auc |>
  ggplot(aes(x = loss, y = classif.auc, color = learner_name)) +
  facet_wrap(vars(task_name)) +
  geom_point(size = 2, alpha = .75) +
  scale_color_brewer(palette = "Dark2") +
  labs(
    title = "Binary classifification tuning results",
    subtitle = "Tuned on AUC"
  ) +
  theme_minimal() +
  theme(
    panel.spacing.x = unit(1, "cm"),
    legend.position = "bottom"
  )

tuning_results_rpf_binary_auc |>
  ggplot(aes(x = split_try, y = classif.auc, color = learner_name)) +
  facet_wrap(vars(task_name)) +
  geom_point(size = 2, alpha = .75) +
  scale_color_brewer(palette = "Dark2") +
  labs(
    title = "Binary classifification tuning results",
    subtitle = "Tuned on AUC"
  ) +
  theme_minimal() +
  theme(
    panel.spacing.x = unit(1, "cm"),
    legend.position = "bottom"
  )

tuning_results_rpf_binary_auc |>
  ggplot(aes(x = t_try, y = classif.auc, color = learner_name)) +
  facet_wrap(vars(task_name)) +
  geom_point(size = 2, alpha = .75) +
  scale_color_brewer(palette = "Dark2") +
  labs(
    title = "Binary classifification tuning results",
    subtitle = "Tuned on AUC"
  ) +
  theme_minimal() +
  theme(
    panel.spacing.x = unit(1, "cm"),
    legend.position = "bottom"
  )
```


## Multiclass Classification

- Tuned and evaluation on:
    -  [AUNP](https://mlr3.mlr-org.com/reference/mlr_measures_classif.mauc_aunp.html)
    -  [Multiclass Brier](https://mlr3.mlr-org.com/reference/mlr_measures_classif.mbrier.html)

### Tasks

[OpenML CC18](https://www.openml.org/search?type=study&study_type=task&id=99) tasks with:

- Multiclass target
- No missing values
- $n \cdot p \leq 10^5$

```{r multiclass-tasks}
task_summary |>
  filter(!twoclass & dim <= 1e5) |>
  select(task_id, task_name, n, p, dim) |>
  kbl(
    caption = "Selected OpenML CC18 tasks for multiclass classification",
    col.names = c("ID", "Task", "n", "p", "np"),
    booktabs = TRUE
  ) |>
  kable_styling()
```

### Aggregated Results

#### Boxplot

```{r multiclass-aggregated}
#| fig-width: 12
#| fig-height: 7
#| fig-align: center
#| column: screen-inset-shaded

multiclass_aggrs |>
  ggplot(aes(x = learner_name, y = score, fill = learner_name)) +
    facet_wrap(vars(measure), ncol = 2, scales = "free") +
    geom_boxplot(alpha = .5) +
    geom_point(position = position_jitterdodge(dodge.width = .25)) +
    scale_y_continuous(labels = scales::percent) +
    #scale_x_discrete(guide = guide_axis(n.dodge = 2)) +
    #coord_cartesian(ylim = c(0, 1)) +
    #coord_flip() +
    scale_fill_manual(values = learner_cols, guide = "none") +
    labs(
      title = "Multiclass Classification: Aggregated scores over all tasks",
      x = NULL, y = "Score"
    ) +
    theme_minimal(base_size = 14)
```

#### Table

```{r multiclass-aggregated-table}
# column: screen-inset-shaded
multiclass_aggrs |>
  group_by(learner_name, measure) |>
  summarize(
    mean = mean(score),
    median = median(score),
    sd = sd(score),
    q25 = quantile(score, .25),
    q75 = quantile(score, .75),
    .groups = "drop"
  ) |>
  mutate(across(where(is.numeric), \(x) round(100 * x, 1))) |>
  mutate(
    mean = glue::glue("{mean} [{sd}]"),
    median = glue::glue("{median} [{q25}; {q75}]")
  ) |>
  select(-sd, -q25, -q75) |>
  tidyr::pivot_wider(names_from = measure, names_sep = "_", values_from = mean:median) |>
  select(learner_name, contains("AUC"), contains("Brier")) |>
  kbl(
    col.names = c("Learner", rep(c("Mean [SD]", "Median [q25; q75]"), 2)),
    booktabs = TRUE
  ) |>
  kable_styling(full_width = TRUE, latex_options = c("HOLD_position")) |>
  add_header_above(header = c(" " = 1, "AUNP (%)" = 2, "Multiclass Brier Score (%)" = 2)) |>
  column_spec(1, "4cm") |>
  column_spec(2, "2.3cm") |>
  column_spec(3, "3cm") |>
  column_spec(4, "2.3cm") |>
  column_spec(5, "3cm")
```


### Results per Task

```{r multiclass-per-task-auc}
#| fig-width: 12
#| fig-height: 7
#| fig-align: center
#| column: screen-inset-shaded

multiclass_scores |>
  filter(measure == "AUC") |>
  ggplot(aes(y = learner_name, x = score, fill = learner_name)) +
  facet_wrap(vars(task_name), scales = "free_x") +
  geom_boxplot(alpha = .5) +
  geom_point() +
  scale_fill_manual(values = learner_cols, guide = "none") +
  scale_x_continuous(labels = scales::percent) +
  labs(
    title = "Multiclass scores per task",
    subtitle = "Tasks ordered by n * p, decreasing",
    y = "Learner", x = "AUNP (%)"
  ) +
  theme_minimal() +
  theme(
    panel.spacing.x = unit(1, "cm")
  )
```


```{r multiclass-per-task-brier}
#| fig-width: 12
#| fig-height: 7
#| fig-align: center
#| column: screen-inset-shaded

multiclass_scores |>
  filter(measure == "Brier Score") |>
  ggplot(aes(y = learner_name, x = score, fill = learner_name)) +
  facet_wrap(vars(task_name), scales = "free") +
  geom_boxplot(alpha = .5) +
  geom_point() +
  scale_fill_manual(values = learner_cols, guide = "none") +
  scale_x_continuous(labels = scales::percent) +
  labs(
    title = "Scores per task",
    subtitle = "Tasks ordered by n * p, decreasing",
    y = "Learner", x = "Multiclass Brier Score (%)"
  ) +
  theme_minimal() +
  theme(
    panel.spacing.x = unit(1, "cm")
  )
```

### Tuning Results

Showing best performing parameter combinations of the inner folds, e.g. the parameters that "won".
One plot per hyperparameter, with performance on y-axis.

Note that `max_interaction` is tuned by tuning `max_interaction_ratio` within [0, 1] and then calculating

$$\max(\lceil \mathtt{max\_interaction\_ratio} \cdot \min(p, 30)\rceil, 1)$$
to effectively tune `max_interaction` within 1 and $p$ or 30, whichever is lower. 
This is not ideal since multiple values for `max_interaction_ratio` could result in the same value for `max_interaction` for large $p$, but it's a compromise.

```{r multiclass-tuning-resutls-auc}
#| fig-width: 12
#| fig-height: 7
#| fig-align: center
#| column: screen-inset-shaded

tuning_results_rpf_multiclass_auc |>
  ggplot(aes(x = splits, y = classif.mauc_aunp, color = learner_name)) +
  facet_wrap(vars(task_name)) +
  geom_point(size = 2, alpha = .75) +
  scale_color_brewer(palette = "Dark2") +
  labs(
    title = "Multiclass classifification tuning results",
    subtitle = "Tuned on AUC"
  ) +
  theme_minimal() +
  theme(
    panel.spacing.x = unit(1, "cm"),
    legend.position = "bottom"
  )

tuning_results_rpf_multiclass_auc |>
  #mutate(task_name = glue::glue("{task_name} (p={p})")) |>
  ggplot(aes(x = max_interaction, y = classif.mauc_aunp, color = learner_name)) +
  facet_wrap(vars(task_name)) +
  geom_point(size = 2, alpha = .75) +
  scale_color_brewer(palette = "Dark2") +
  labs(
    title = "Multiclass classifification tuning results",
    subtitle = "Tuned on AUC"
  ) +
  theme_minimal() +
  theme(
    panel.spacing.x = unit(1, "cm"),
    legend.position = "bottom"
  )

tuning_results_rpf_multiclass_auc |>
  ggplot(aes(x = loss, y = classif.mauc_aunp, color = learner_name)) +
  facet_wrap(vars(task_name)) +
  geom_point(size = 2, alpha = .75) +
  scale_color_brewer(palette = "Dark2") +
  labs(
    title = "Multiclass classifification tuning results",
    subtitle = "Tuned on AUC"
  ) +
  theme_minimal() +
  theme(
    panel.spacing.x = unit(1, "cm"),
    legend.position = "bottom"
  )

tuning_results_rpf_multiclass_auc |>
  ggplot(aes(x = split_try, y = classif.mauc_aunp, color = learner_name)) +
  facet_wrap(vars(task_name)) +
  geom_point(size = 2, alpha = .75) +
  scale_color_brewer(palette = "Dark2") +
  labs(
    title = "Multiclass classifification tuning results",
    subtitle = "Tuned on AUC"
  ) +
  theme_minimal() +
  theme(
    panel.spacing.x = unit(1, "cm"),
    legend.position = "bottom"
  )

tuning_results_rpf_multiclass_auc |>
  ggplot(aes(x = t_try, y = classif.mauc_aunp, color = learner_name)) +
  facet_wrap(vars(task_name)) +
  geom_point(size = 2, alpha = .75) +
  scale_color_brewer(palette = "Dark2") +
  labs(
    title = "Multiclass classifification tuning results",
    subtitle = "Tuned on AUC"
  ) +
  theme_minimal() +
  theme(
    panel.spacing.x = unit(1, "cm"),
    legend.position = "bottom"
  )
```
